{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Attack - solution guidelines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvK6LaVhrfZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "from collections import OrderedDict\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05dkTKAsEh-",
        "colab_type": "code",
        "outputId": "f71a3657-eecd-46f1-d5c9-1b538450e5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# project_path = '~/.pytorch/CIFAR10_data/'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZthQfTYDkPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = '/content/drive/My Drive/cifar10/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW1mLverl7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "transform = transforms.Compose(\n",
        "                        [transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsFJTFwr1wV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "63d85595-d3c1-487a-b87b-6217ce10b494"
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# load the test data\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMO85RUwF4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "15c0772f-2a2f-40e6-8aed-093a7fcd6781"
      },
      "source": [
        "testset[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.2392,  0.2471,  0.2941,  ...,  0.0745, -0.0118, -0.0902],\n",
              "          [ 0.1922,  0.1843,  0.2471,  ...,  0.0667, -0.0196, -0.0667],\n",
              "          [ 0.1843,  0.1843,  0.2392,  ...,  0.0902,  0.0196, -0.0588],\n",
              "          ...,\n",
              "          [-0.4667, -0.6706, -0.7569,  ..., -0.7020, -0.8980, -0.6863],\n",
              "          [-0.5216, -0.6157, -0.7255,  ..., -0.7961, -0.7725, -0.8431],\n",
              "          [-0.5765, -0.5608, -0.6471,  ..., -0.8118, -0.7333, -0.8353]],\n",
              " \n",
              "         [[-0.1216, -0.1294, -0.0902,  ..., -0.2549, -0.2863, -0.3333],\n",
              "          [-0.1216, -0.1373, -0.1059,  ..., -0.2549, -0.2863, -0.3098],\n",
              "          [-0.1373, -0.1451, -0.1294,  ..., -0.2314, -0.2549, -0.3020],\n",
              "          ...,\n",
              "          [-0.0275, -0.2157, -0.3098,  ..., -0.2392, -0.4980, -0.3333],\n",
              "          [-0.0902, -0.2000, -0.3333,  ..., -0.3569, -0.3569, -0.4980],\n",
              "          [-0.1608, -0.1765, -0.3020,  ..., -0.3961, -0.3412, -0.4745]],\n",
              " \n",
              "         [[-0.6157, -0.6314, -0.6000,  ..., -0.7176, -0.7176, -0.7412],\n",
              "          [-0.6000, -0.6863, -0.6471,  ..., -0.7569, -0.7490, -0.7333],\n",
              "          [-0.6314, -0.7412, -0.7176,  ..., -0.7333, -0.7333, -0.7412],\n",
              "          ...,\n",
              "          [ 0.3882,  0.1608,  0.0745,  ...,  0.1451, -0.1529, -0.0039],\n",
              "          [ 0.3176,  0.1608,  0.0353,  ...,  0.0196, -0.0118, -0.1608],\n",
              "          [ 0.2549,  0.1686,  0.0353,  ..., -0.0275,  0.0118, -0.1373]]]), 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itY7G_y3tDnC",
        "colab_type": "code",
        "outputId": "7b337fe5-e705-4c7a-81d0-c22f459a6ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# helper function to unnormalize and plot image \n",
        "def imshow(img):\n",
        "    img = np.array(img)\n",
        "    img = img / 2 + 0.5\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  car horse truck plane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJK32k42gbEK",
        "colab_type": "code",
        "outputId": "79a23f86-d811-4adf-9d15-5257764c1f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size = len(trainset)\n",
        "split1 = total_size // 4\n",
        "split2 = split1 * 2\n",
        "split3 = split1 * 3\n",
        "\n",
        "print(total_size, split1, split2, split3)\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices[:split1]\n",
        "shadow_out_idx = indices[split1:split2]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices[split2:split3]\n",
        "target_out_idx = indices[split3:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 12500 25000 37500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wJ_0lkhp76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_out_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9LuQJNuCXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWj6JwJOR0u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3Hj738cBPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_B4V8YjKyU",
        "colab_type": "code",
        "outputId": "4a6ffde2-a957-459a-d521-37a3848eecc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "# initalize a target model and train it\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(target_model.parameters(), lr=0.001)\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(50):  \n",
        "\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(target_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        output = target_model(inputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # updaate weights in backprop\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "            \n",
        "\n",
        "    # print statistics\n",
        "    print('Epoch {} loss {}'.format(epoch, loss))\n",
        "        \n",
        "        \n",
        "\n",
        "print('Finished Training the Target model...')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 2.287034511566162\n",
            "Epoch 1 loss 2.2235891819000244\n",
            "Epoch 2 loss 2.222200393676758\n",
            "Epoch 3 loss 2.401151180267334\n",
            "Epoch 4 loss 1.5556485652923584\n",
            "Epoch 5 loss 1.9683756828308105\n",
            "Epoch 6 loss 1.7281196117401123\n",
            "Epoch 7 loss 1.8967417478561401\n",
            "Epoch 8 loss 1.515014886856079\n",
            "Epoch 9 loss 1.3012360334396362\n",
            "Epoch 10 loss 2.0363047122955322\n",
            "Epoch 11 loss 2.0609304904937744\n",
            "Epoch 12 loss 0.8777512311935425\n",
            "Epoch 13 loss 1.6446969509124756\n",
            "Epoch 14 loss 1.70819091796875\n",
            "Epoch 15 loss 1.2218401432037354\n",
            "Epoch 16 loss 1.7837920188903809\n",
            "Epoch 17 loss 1.5525791645050049\n",
            "Epoch 18 loss 1.687563180923462\n",
            "Epoch 19 loss 1.1957896947860718\n",
            "Epoch 20 loss 1.4775322675704956\n",
            "Epoch 21 loss 2.884528636932373\n",
            "Epoch 22 loss 1.2245359420776367\n",
            "Epoch 23 loss 1.1168580055236816\n",
            "Epoch 24 loss 1.4372315406799316\n",
            "Epoch 25 loss 0.9721407294273376\n",
            "Epoch 26 loss 0.6936682462692261\n",
            "Epoch 27 loss 1.933525800704956\n",
            "Epoch 28 loss 1.8137125968933105\n",
            "Epoch 29 loss 0.26816606521606445\n",
            "Epoch 30 loss 1.2761279344558716\n",
            "Epoch 31 loss 1.020085334777832\n",
            "Epoch 32 loss 1.8045158386230469\n",
            "Epoch 33 loss 1.0677335262298584\n",
            "Epoch 34 loss 1.9599953889846802\n",
            "Epoch 35 loss 1.1621977090835571\n",
            "Epoch 36 loss 0.3990745544433594\n",
            "Epoch 37 loss 1.8017356395721436\n",
            "Epoch 38 loss 0.24150443077087402\n",
            "Epoch 39 loss 0.35834717750549316\n",
            "Epoch 40 loss 1.1208913326263428\n",
            "Epoch 41 loss 0.3737797737121582\n",
            "Epoch 42 loss 0.7489876747131348\n",
            "Epoch 43 loss 1.8197038173675537\n",
            "Epoch 44 loss 0.9450222253799438\n",
            "Epoch 45 loss 1.1313855648040771\n",
            "Epoch 46 loss 1.7213412523269653\n",
            "Epoch 47 loss 0.9297528266906738\n",
            "Epoch 48 loss 1.1957707405090332\n",
            "Epoch 49 loss 0.1495799422264099\n",
            "Finished Training the Target model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpY8ktdskRQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67ebe826-1bc3-4833-f612-3f8a9658f7ba"
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in target_out_loader:\n",
        "        images, labels = data\n",
        "        outputs = target_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 53 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEClKFqikmUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt4L48eroR3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(target_model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKgFPagxqr3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "673a7fa7-63b3-44d7-f9cc-1e8c25438810"
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "target_model.load_state_dict(state_dict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_VqRVVkoCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "5267984e-47ca-48a1-e5a4-56d969705526"
      },
      "source": [
        "# initalize a Shadow Model and Train it\n",
        "# for the first ICP, your shadow model can have the same CNN architecture and hyperparameters\n",
        "\n",
        "shadow_model = Net()\n",
        "shadow_criterion = nn.CrossEntropyLoss()\n",
        "shadow_optimizer = optim.SGD(shadow_model.parameters(), lr=0.001)\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(50):  \n",
        "\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(shadow_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        shadow_optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        output = shadow_model(inputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # updaate weights in backprop\n",
        "        shadow_optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "            \n",
        "\n",
        "    # print statistics\n",
        "    print('Epoch {} loss {}'.format(epoch, loss))\n",
        "\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 2.3352644443511963\n",
            "Epoch 1 loss 2.29719877243042\n",
            "Epoch 2 loss 2.2538747787475586\n",
            "Epoch 3 loss 2.221916675567627\n",
            "Epoch 4 loss 2.962458610534668\n",
            "Epoch 5 loss 1.66731858253479\n",
            "Epoch 6 loss 1.8071935176849365\n",
            "Epoch 7 loss 1.790611982345581\n",
            "Epoch 8 loss 1.6060004234313965\n",
            "Epoch 9 loss 1.6123257875442505\n",
            "Epoch 10 loss 1.837048053741455\n",
            "Epoch 11 loss 1.2671312093734741\n",
            "Epoch 12 loss 1.5290412902832031\n",
            "Epoch 13 loss 1.9907810688018799\n",
            "Epoch 14 loss 1.1075338125228882\n",
            "Epoch 15 loss 1.090780258178711\n",
            "Epoch 16 loss 1.0936388969421387\n",
            "Epoch 17 loss 1.2463314533233643\n",
            "Epoch 18 loss 1.8890342712402344\n",
            "Epoch 19 loss 0.9045959115028381\n",
            "Epoch 20 loss 0.9605705142021179\n",
            "Epoch 21 loss 1.135425090789795\n",
            "Epoch 22 loss 1.8436710834503174\n",
            "Epoch 23 loss 0.49497413635253906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-28fb7852b788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadow_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# see RawEncode.c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95hpA5UeqSRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(shadow_model.state_dict(), 'shadow_checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEsV3xJwppIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9defc9bb-6215-4e3d-97e6-6e0f155e812c"
      },
      "source": [
        "# calculate the accuracy of the Shadow Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in shadow_out_loader:\n",
        "        images, labels = data\n",
        "        outputs = shadow_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 49 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kP_-62ljFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze the Shadow model \n",
        "shadow_model.eval()\n",
        "\n",
        "for param in shadow_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "shadow_data = []\n",
        "\n",
        "for inputs, labels in shadow_train_loader:\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    shadow_data.append((predictions[i], 1))\n",
        "  \n",
        "for inputs, labels in shadow_out_loader:\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    shadow_data.append((predictions[i], 0))\n",
        "\n",
        "# create a new dataset of the shape [predictions(shadow_in), 1], [predicitons(shadow_out), 1] and zip them together\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fl38p2AwXR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attack dataloader\n",
        "attack_train_loader = torch.utils.data.DataLoader(shadow_data, batch_size=4, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlbGi07pzb0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "attack_model = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(10, 5)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(5, 2))\n",
        "                          ]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymKnj7QpdDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "520b893f-fb4b-474f-97e3-dcb90bb7b519"
      },
      "source": [
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "\n",
        "# initalize a Shadow Model and Train it\n",
        "# for the first ICP, your shadow model can have the same CNN architecture and hyperparameters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "attack_opt = optim.SGD(attack_model.parameters(), lr=0.001)\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(20):  \n",
        "\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(attack_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        attack_opt.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        output = attack_model(inputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # updaate weights in backprop\n",
        "        attack_opt.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "            \n",
        "\n",
        "    # print statistics\n",
        "    print('Epoch {} loss {}'.format(epoch, loss))\n",
        "\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 0.6730378866195679\n",
            "Epoch 1 loss 0.6698541045188904\n",
            "Epoch 2 loss 0.6928651332855225\n",
            "Epoch 3 loss 0.7105040550231934\n",
            "Epoch 4 loss 0.702652096748352\n",
            "Epoch 5 loss 0.6982937455177307\n",
            "Epoch 6 loss 0.7161573767662048\n",
            "Epoch 7 loss 0.7023587226867676\n",
            "Epoch 8 loss 0.6896083354949951\n",
            "Epoch 9 loss 0.694150447845459\n",
            "Epoch 10 loss 0.687923789024353\n",
            "Epoch 11 loss 0.7035459280014038\n",
            "Epoch 12 loss 0.6941201686859131\n",
            "Epoch 13 loss 0.7074728012084961\n",
            "Epoch 14 loss 0.7059978246688843\n",
            "Epoch 15 loss 0.6940752267837524\n",
            "Epoch 16 loss 0.7144498825073242\n",
            "Epoch 17 loss 0.7077149748802185\n",
            "Epoch 18 loss 0.6824926733970642\n",
            "Epoch 19 loss 0.6972743272781372\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cerJOsTW1L4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(attack_model.state_dict(), 'attack_checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1B1OhMp6er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "# and then input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "\n",
        "\n",
        "# Run attack agains the target model\n",
        "actual_value = []\n",
        "pred_value = []\n",
        "\n",
        "attack_model.eval()\n",
        "\n",
        "for param in attack_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for inputs, labels in target_train_loader:\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(1)\n",
        "    \n",
        "for inputs, labels in target_out_loader:\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ykX5nB5Olu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(actual_value)):\n",
        "  print(\"Actual: {}   Prediction: {}\".format(actual_value[i], pred_value[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BsW9xY66x8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "32b83f52-0184-4451-d81a-35bf219e3fbc"
      },
      "source": [
        "# Calculate recall and precision\n",
        "# precison = true positive / true positive + false positive\n",
        "true_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "total_positive = sum(pred_value)\n",
        "for i in range(len(actual_value)):\n",
        "  if (pred_value[i] == 1) and (actual_value[i] == 1):\n",
        "    true_pos += 1\n",
        "  elif (pred_value[i] == 0 and actual_value[i] == 1):\n",
        "    false_neg += 1\n",
        "    \n",
        "print('True positive: {} Total Positive: {} Precision: {}'.format(true_pos, total_positive, true_pos / total_positive))\n",
        "print('Recall: {}', true_pos / (true_pos + false_neg))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True positive: 8892 Total Positive: 18040 Precision: 0.4929046563192905\n",
            "Recall: {} 0.71136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsyCgziqeaa",
        "colab_type": "text"
      },
      "source": [
        "Great! At this point, you must have created a succesfful attack model that can detect whether a datapoint was used in training a target mode or not. \n",
        "* A successful attack model is one with a precision/recall higher than 85% -- you are using same architecture and are aware of the data classes\n",
        "\n",
        " \n",
        " Can you suggest any defense mechanism? If yes, Apply them to your solution and re-evaluate your attack model. How did your defense mecanism affect the accuracy of the target model? How did it affect the recall and precision of the Attack model?"
      ]
    }
  ]
}